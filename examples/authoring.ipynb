{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authoring Hub Predictors\n",
    "NatML provides infrastructure that allows businesses to very quickly deploy machine learning in their products. NatML handles all infrastructure needs, including server provisioning, data transfers, and auto-scaling. As a result, all developers need to do is simply bring their model, along with lightweight code for making predictions server- and client-side. There are two components involved:\n",
    "\n",
    "1. **Executor**: The `MLExecutor` is code that is responsible for running the user's ML models on the NatML Hub cloud infrastructure. The executor is responsible for instantiating an inference runtime (like ONNX Runtime or PyTorch) and running inference on features provided by the NatML framework from end-users.\n",
    "\n",
    "2. **Predictor**: The predictor is a lightweight component that end-users use to request predictions. The predictor submits prediction workloads to the NatML Hub platform, and post-processes the results client-side into a form easily usable by the end-user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Executor\n",
    "NatML provides the `MLExecutor` class which all executors must derive from. We will write an executor for the MobileNet v2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natml import MLExecutor, MLModelData, MLFeature\n",
    "from onnxruntime import InferenceSession\n",
    "from typing import List\n",
    "\n",
    "class MobileNetv2Executor (MLExecutor):\n",
    "\n",
    "    def initialize (self, model_data: MLModelData, graph_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the executor.\n",
    "        \n",
    "        This method should instantiate an ML inference session using an\n",
    "        inference framework like ONNX Runtime, PyTorch, or TensorFlow.\n",
    "\n",
    "        Parameters:\n",
    "            model_data (MLModelData): Model data.\n",
    "            graph_path (str): Path to ML graph on the file system.\n",
    "        \"\"\"\n",
    "        self.__session = InferenceSession(graph_path)\n",
    "        self.__model_data = model_data\n",
    "\n",
    "    def predict (self, *inputs: List[MLFeature]) -> List[MLFeature]:\n",
    "        \"\"\"\n",
    "        Make a prediction on one or more input features.\n",
    "\n",
    "        Parameters:\n",
    "            inputs (list): Input features.\n",
    "\n",
    "        Returns:\n",
    "            list: Output features.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natml import MLModelData, MLModel, MLFeature\n",
    "from typing import List\n",
    "\n",
    "class MobileNetv2HubPredictor:\n",
    "\n",
    "    def __init__ (self, model: MLModel):\n",
    "        self.__model = model\n",
    "\n",
    "    def predict (self, *inputs: List[MLFeature]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'deserialize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/98/2x4_h0694k17yxw6khx3tmx80000gn/T/ipykernel_70391/501989906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLModelData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@natsuite/mobilenet-v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Deserialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMobileNetv2HubPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'deserialize'"
     ]
    }
   ],
   "source": [
    "# Fetch the model data from Hub\n",
    "access_key = \"<HUB ACCESS KEY>\"\n",
    "model_data = MLModelData.from_hub(\"@natsuite/mobilenet-v2\", access_key)\n",
    "# Deserialize the model\n",
    "model = model_data.deserialize()\n",
    "# Create the predictor\n",
    "predictor = MobileNetv2HubPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
